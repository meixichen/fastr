% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/leave_one_out_predict.R
\name{leave_one_out_predict}
\alias{leave_one_out_predict}
\title{Leave-neuron-out posterior prediction.}
\usage{
leave_one_out_predict(
  mod_fit,
  left_out_neuron,
  data,
  predict_method = c("mvn_joint", "mvn_2step_marg", "mvn_2step_both"),
  fix_marg_param = TRUE,
  woodbury = TRUE,
  lam = NULL,
  nu = 15,
  silent = TRUE,
  init_param = NULL,
  integrate_random = TRUE,
  adfun_only = FALSE
)
}
\arguments{
\item{mod_fit}{The fitted model of class \code{fastr_fit}.}

\item{left_out_neuron}{Nonzero integer index for the neuron to be predicted.}

\item{data}{A \verb{n_cell x n_bin x n_trial} (multiple neurons) array of binary
spike trains.}

\item{predict_method}{Character string "mvn_joint", "mvn_2step_marg", or
"mvn_2step_both". See details.}

\item{fix_marg_param}{Are the marginal parameters (k and alpha) treated as
fixed? Default is TRUE. When k and alpha are fixed, only the loading matrix
is sampled. See details.}

\item{woodbury}{Should the Woodbury matrix identity be used for covariance
matrix operations? Default is TRUE.}

\item{lam}{Scalar. Penalization parameter for the loading matrix elements.
Default is \code{lam=ifelse(n_cell<10, 1, 0.5)}.}

\item{nu}{Scalar. Parameter that controls the "steepness" around 0 of the sigmoid
function applied at the spike data likelihood layer. Default is 15.}

\item{silent}{Suppress model fitting messages?}

\item{init_param}{Optional. A list of initial parameters for constructing the
model template (ADfun).}

\item{integrate_random}{Integrate random effects? If TRUE, latent paths are
integrated out in the model. Setting to FALSE can be helpful for checking the
marginal likelihood.}

\item{adfun_only}{Only outputs of ADFun created by TMB? This is for debugging
purposes only.}
}
\value{
A function that can be run iteratively to sample from the posterior
predictive distribution.
}
\description{
Leave-neuron-out posterior prediction.
}
\details{
Three method are available for leave-neuron-out posterior prediction. They
are listed below in terms of preference related to computational cost.
\enumerate{
\item \code{mvn_joint} samples parameters and latent paths jontly from a MVN:
\deqn{(\theta, X) \sim MVN\Big((\hat{\theta}, \hat{X}(\hat{\theta})),
\hat{\Sigma}_{\mathrm{joint}}\Big)}
where \eqn{\hat{X}(\hat{\theta})} is a first order Taylor expansion of
\eqn{X(\theta)} around \eqn{\hat{\theta}}, and
\eqn{\hat{\Sigma}_{\mathrm{joint}}} is a zero-th order Taylor expansion of
the joint covariance at \eqn{\hat{\theta}}.
\item \code{mvn_2step_marg} samples the paths in two steps. First, sample
\eqn{\theta^s \sim q(\theta \mid y)}, where \eqn{q()}
indicates an approximate posterior.
Then conditional on \eqn{\theta^s} value, set \eqn{X^s = \hat{X}(\hat{\theta})}.
I.e., only the parameters are sampled randomly whereas the paths are obtained
deterministically.
\item \code{mvn_2step_both} samples the paths in two steps but with both steps
performing random sampling. The first step is the same as that for
\code{mvn_2step_marg}. In the second step, sample the latent paths
\eqn{X^s \sim MVN\Big(\hat{X}(\hat{\theta}), \hat{\Sigma}(\hat{\theta})\Big)}.
I.e., both parameters and paths are sampled randomly.
}

In terms of computational costs, methods 2 and 3 are similar in speed since
the additional cost from randomly sampling step 2 is not massive. That said,
method 1 is preferred since it is much faster.
}
